{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from similar_pairs import * \n",
    "from embedding_visualization import * \n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GloVe embeddings ## \n",
    "Word embeddings used here are pre-trained GloVe embeddings, which can be downloaded \n",
    "[here](https://nlp.stanford.edu/projects/glove/). \n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load pre-trained on 6 billion tokens GloVe embeddings \n",
    "small_embeddings = load_word_vector(6)\n",
    "# Load pre-trained on 840 billion tokens GloVe embeddings\n",
    "large_embeddings = load_word_vector(840)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualize word embeddings in reduced dimension ##\n",
    "GloVe embeddings used in this example are high-dimensional, thus it is difficult to visualize them. To see the \n",
    "relationships between word vectors, it's important to perform dimensionality reduction by using algorithms such as t-SNE.\n",
    "In this example, I use [scikit-learn's implementation of t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# List of words to visualize\n",
    "words = ['france', 'england', 'spain', 'dog', 'cat', 'mother', 'father', 'brother', 'sister']\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize chosen words using the smaller pre-trained embeddings.\n",
    "visualize(words, small_embeddings)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize chosen words using the larger pre-trained embeddings. \n",
    "visualize(words, large_embeddings)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It's observable that the larger GloVe embeddings are able to represent the relationships among the chosen words\n",
    "better, as words with semantic and/or syntactic meanings are grouped together in the graph. More specifically, \n",
    "France, England, and Spain are in one group (countries); dog and cat are in one group (animals/pets); mother, \n",
    "father, brother, and sister are in one group (family members/people). \n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Complete analogies ## \n",
    "Since word embeddings are able to learn and represent relationships among words, it is possible that they can be used \n",
    "to finish analogies such as: \"Man is to King as Woman is to ___\", where the answer should be \"Queen\".  \n",
    "For this task, the answer is given by the formula:  \n",
    "E(Man) - E(King) $\\approx$ E(Woman) - E(?)  \n",
    "$\\Rightarrow$ E(?) = E(Woman) - E(Man) + E(King)  \n",
    "If the embeddings are learned properly, the result of E(?) should be close to E(Queen). \n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Lists of inputs\n",
    "input_1 = [\"man\", \"king\", \"woman\"]\n",
    "input_2 = [\"man\", \"waiter\", \"woman\"]\n",
    "input_3 = [\"london\", \"england\", \"paris\"]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Outputs \n",
    "output_1 = finish_similar_pairs(input_1, large_embeddings)\n",
    "output_2 = finish_similar_pairs(input_2, large_embeddings)\n",
    "output_3 = finish_similar_pairs(input_3, large_embeddings)\n",
    "print_sentence(input_1, output_1)\n",
    "print_sentence(input_2, output_2)\n",
    "print_sentence(input_3, output_3)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}